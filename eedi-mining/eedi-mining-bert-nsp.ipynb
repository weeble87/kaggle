{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:27.862782Z",
     "iopub.status.busy": "2024-11-30T19:33:27.862391Z",
     "iopub.status.idle": "2024-11-30T19:33:27.872934Z",
     "shell.execute_reply": "2024-11-30T19:33:27.871633Z",
     "shell.execute_reply.started": "2024-11-30T19:33:27.862739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logging.info(\"starting script\")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from logging import getLogger, Formatter, FileHandler, StreamHandler, INFO, DEBUG\n",
    "\n",
    "\n",
    "def create_logger(exp_version):\n",
    "    log_file = (\"{}.log\".format(exp_version))\n",
    "\n",
    "    # logger\n",
    "    logger_ = getLogger(exp_version)\n",
    "    logger_.setLevel(DEBUG)\n",
    "\n",
    "    # formatter\n",
    "    fmr = Formatter(\"[%(levelname)s] %(asctime)s >>\\t%(message)s\")\n",
    "\n",
    "    # file handler\n",
    "    fh = FileHandler(log_file)\n",
    "    fh.setLevel(DEBUG)\n",
    "    fh.setFormatter(fmr)\n",
    "\n",
    "    # stream handler\n",
    "    ch = StreamHandler()\n",
    "    ch.setLevel(INFO)\n",
    "    ch.setFormatter(fmr)\n",
    "\n",
    "    logger_.addHandler(fh)\n",
    "    logger_.addHandler(ch)\n",
    "\n",
    "\n",
    "def get_logger(exp_version):\n",
    "    return getLogger(exp_version)\n",
    "\n",
    "VERSION = \"001\" \n",
    "create_logger(VERSION)\n",
    "get_logger(VERSION).info(\"top of script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:27.875417Z",
     "iopub.status.busy": "2024-11-30T19:33:27.874953Z",
     "iopub.status.idle": "2024-11-30T19:33:28.239861Z",
     "shell.execute_reply": "2024-11-30T19:33:28.238601Z",
     "shell.execute_reply.started": "2024-11-30T19:33:27.875369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "files = []\n",
    "data_dir = 'C:/ai_ml/kaggle/eedi-mining/eedi-mining-misconceptions-in-mathematics'\n",
    "miscon_file_index = 0\n",
    "train_file_index = 3\n",
    "test_file_index = 2\n",
    "# on kaggle\n",
    "# data_dir = '/kaggle/input'\n",
    "#miscon_file_index = 1\n",
    "#train_file_index = 2\n",
    "#test_file_index = 3\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        get_logger(VERSION).info(os.path.join(dirname, filename))\n",
    "        files.append(os.path.join(dirname, filename))\n",
    "print(files)\n",
    "misconceptions_filename = files[miscon_file_index]\n",
    "train_filename = files[train_file_index]\n",
    "test_filename = files[test_file_index]\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and Field Information\n",
    "### [train/test].csv\n",
    "\n",
    "* QuestionId - Unique question identifier (int).\n",
    "* ConstructId - Unique construct identifier (int) .\n",
    "* ConstructName - Most granular level of knowledge related to question (str).\n",
    "* CorrectAnswer - A, B, C or D (char).\n",
    "* SubjectId - Unique subject identifier (int).\n",
    "* SubjectName - More general context than the construct (str).\n",
    "* QuestionText - Question text extracted from the question image using human-in-the-loop OCR (str) .\n",
    "* Answer[A/B/C/D]Text - Answer option A text extracted from the question image using human-in-the-loop OCR (str).\n",
    "* Misconception[A/B/C/D]Id - Unique misconception identifier (int). Ground truth labels in train.csv; your task is to predict these labels for test.csv.\n",
    "\n",
    "### misconception_mapping.csv\n",
    "maps MisconceptionId to its MisconceptionName\n",
    "\n",
    "### sample_submission.csv\n",
    "A submission file in the correct format.\n",
    "* QuestionId_Answer - Each question has three incorrect answers for which need you predict the MisconceptionId.\n",
    "* MisconceptionId - You can predict up to 25 values, space delimited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.242955Z",
     "iopub.status.busy": "2024-11-30T19:33:28.242186Z",
     "iopub.status.idle": "2024-11-30T19:33:28.315291Z",
     "shell.execute_reply": "2024-11-30T19:33:28.313987Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.242885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_filename)\n",
    "train_data_orig = train_data.copy()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.317292Z",
     "iopub.status.busy": "2024-11-30T19:33:28.316813Z",
     "iopub.status.idle": "2024-11-30T19:33:28.331231Z",
     "shell.execute_reply": "2024-11-30T19:33:28.329836Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.317244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test_filename)\n",
    "test_data_orig = test_data.copy()\n",
    "test_data.head()\n",
    "print (\"test_data shape\")\n",
    "print(\"rows\", test_data.shape[0])\n",
    "print(\"columns\",test_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get misconception data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.335159Z",
     "iopub.status.busy": "2024-11-30T19:33:28.334678Z",
     "iopub.status.idle": "2024-11-30T19:33:28.360948Z",
     "shell.execute_reply": "2024-11-30T19:33:28.359708Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.335112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "miscon_data = pd.read_csv(misconceptions_filename)\n",
    "miscon_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.363158Z",
     "iopub.status.busy": "2024-11-30T19:33:28.362686Z",
     "iopub.status.idle": "2024-11-30T19:33:28.369932Z",
     "shell.execute_reply": "2024-11-30T19:33:28.368694Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.363111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print (\"miscon_data shape\")\n",
    "print(\"rows\", miscon_data.shape[0])\n",
    "print(\"columns\",miscon_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.371812Z",
     "iopub.status.busy": "2024-11-30T19:33:28.371294Z",
     "iopub.status.idle": "2024-11-30T19:33:28.420352Z",
     "shell.execute_reply": "2024-11-30T19:33:28.419198Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.371764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "subject_groups = train_data.groupby([\"SubjectId\",\"SubjectName\"]).count().sort_values('QuestionId',ascending=False).reset_index()\n",
    "print(subject_groups.shape[0], \" different subject groups\")\n",
    "subject_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.422364Z",
     "iopub.status.busy": "2024-11-30T19:33:28.422017Z",
     "iopub.status.idle": "2024-11-30T19:33:28.447183Z",
     "shell.execute_reply": "2024-11-30T19:33:28.445880Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.422331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"percentages of null values\")\n",
    "pd.DataFrame({'Count':train_data.isnull().sum()[train_data.isnull().sum()>0],'Percentage':(train_data.isnull().sum()[train_data.isnull().sum()>0]/train_data.shape[0])*100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.448802Z",
     "iopub.status.busy": "2024-11-30T19:33:28.448459Z",
     "iopub.status.idle": "2024-11-30T19:33:28.473173Z",
     "shell.execute_reply": "2024-11-30T19:33:28.471734Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.448768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# records where more than one misconception id is missing\n",
    "missing_a_misconception = train_data[(train_data.CorrectAnswer == \"A\") & (train_data.MisconceptionBId.isnull() | train_data.MisconceptionCId.isnull()  | train_data.MisconceptionDId.isnull()) ]\n",
    "print(missing_a_misconception.shape)\n",
    "missing_a_misconception.head()\n",
    "train_data[(train_data.CorrectAnswer == \"A\")].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.476111Z",
     "iopub.status.busy": "2024-11-30T19:33:28.475592Z",
     "iopub.status.idle": "2024-11-30T19:33:28.502822Z",
     "shell.execute_reply": "2024-11-30T19:33:28.501598Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.476062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "missing_b_misconpception = train_data[(train_data.CorrectAnswer == \"B\") & (train_data.MisconceptionAId.isnull() | train_data.MisconceptionCId.isnull()) ]\n",
    "missing_b_misconpception.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update data to prepare for creating vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.506841Z",
     "iopub.status.busy": "2024-11-30T19:33:28.506443Z",
     "iopub.status.idle": "2024-11-30T19:33:28.513628Z",
     "shell.execute_reply": "2024-11-30T19:33:28.512236Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.506803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# clean up question\n",
    "# train_data['CleanQuestion'] = train_data['QuestionText'].replace('\\n',' ',regex=True)\n",
    "\n",
    "# create new data frame with just misconceptions\n",
    "# Create a new empty DataFrame\n",
    "miscon_model = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:33:28.516490Z",
     "iopub.status.busy": "2024-11-30T19:33:28.516002Z",
     "iopub.status.idle": "2024-11-30T19:38:05.015769Z",
     "shell.execute_reply": "2024-11-30T19:38:05.014426Z",
     "shell.execute_reply.started": "2024-11-30T19:33:28.516438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# convert latext formatting to text\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "import pylatexenc\n",
    "# Convert LaTeX to plain text\n",
    "# Function to convert LaTeX to text\n",
    "def latex_to_text(latex_string):\n",
    "    return LatexNodes2Text().latex_to_text(latex_string)\n",
    "\"\"\"\n",
    "\n",
    "# repolace names with variables to get consistency on vectors\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "# for running locally, need to do\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to replace all detected names with unique variables\n",
    "def replace_names_with_variables(text):\n",
    "    doc = nlp(text)\n",
    "    name_counter = 1\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            var_name = f\"NAME{name_counter}\"\n",
    "            text = text.replace(ent.text, var_name)\n",
    "            name_counter += 1\n",
    "    return text\n",
    "\n",
    "#def clean_math_text(text):\n",
    "    # clean_text = latex_to_text(text)\n",
    "    # clean_text = replace_names_with_variables(clean_text)\n",
    "    # remove new line and other characters\n",
    "    # print('before',clean_text)\n",
    "    #clean_text = clean_text.replace('\\n',' ')\n",
    "    # print('after', clean_text)\n",
    "    \n",
    "    #return clean_text \n",
    "import re\n",
    "\n",
    "def clean_math_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters (keeping LaTeX symbols)\n",
    "    text = text.replace(\"\\\\n\", \" \")\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\\\{}^_]', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Example text\n",
    "text = \"Here's an example: \\int_0^{\\infty} e^{-x^2} dx.\\n Let's clean it up!\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_math_text(text)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "# remove special characters like new line\n",
    "#miscon_model['subjectName'] = train_data.SubjectName\n",
    "#miscon_model['constructName'] = train_data.ConstructName\n",
    "#miscon_model['question'] = train_data['QuestionText'].apply(clean_math_text)\n",
    "#  Creating the flattened DataFrame\n",
    "import math\n",
    "flattened_data = []\n",
    "next_sentence =[]\n",
    "pairs=[]\n",
    "MAX_LEN=512\n",
    "def add_other_misconceptions_as_label_1(problem_and_wrong_answer, misconception_id_in):\n",
    "    # dataframe of all misconception ids except this on\n",
    "    all_but_one = miscon_data[miscon_data['MisconceptionId'] != misconception_id_in]\n",
    "    all_but_one = all_but_one.sample(n=50, replace=True, random_state=1)\n",
    "    for index0, row0 in all_but_one.iterrows():\n",
    "        #print(\"row0\",row0.MisconceptionName)\n",
    "        next_sentence.append({'problem_and_wrong_answer': problem_and_wrong_answer,\"misconception_text\": row0.MisconceptionName, \"misconception_id\": row0.MisconceptionId,\"label\":1})\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    # put incorrect answer A into row\n",
    "    # print (\"a id\",row.MisconceptionAId)\n",
    "\n",
    "    if ((row.CorrectAnswer != \"A\") & (math.isnan(row.MisconceptionAId) == False)):\n",
    "        misconception_text_row = miscon_data[(miscon_data.MisconceptionId==row.MisconceptionAId)]\n",
    "        #print(type(misconception_text_row))\n",
    "        #print(misconception_text_row.iloc[0,1])\n",
    "        misconception_id = row.MisconceptionAId\n",
    "        formatted_answer = clean_math_text(row['AnswerAText'])\n",
    "        #flattened_data.append({'subjectName': row['SubjectName'], 'constructName': row['ConstructName'], 'latex_formatted_question': row['QuestionText'], 'question': clean_math_text(row['QuestionText']), 'latex_formatted_wrongAnswer': row['AnswerAText'], 'wrongAnswer': clean_math_text(row['AnswerAText']), 'misconception': misconception_row.MisconceptionName})\n",
    "    if ((row.CorrectAnswer != \"B\") & (math.isnan(row.MisconceptionBId)==False)):\n",
    "        misconception_text_row = miscon_data[(miscon_data.MisconceptionId==row.MisconceptionBId)]\n",
    "        misconception_id = row.MisconceptionBId\n",
    "        formatted_answer= clean_math_text(row['AnswerBText'])\n",
    "        # flattened_data.append({'subjectName': row['SubjectName']}, 'constructName': row['ConstructName'], 'latex_formatted_question': row['QuestionText'], 'question': clean_math_text(row['QuestionText']), 'latext_formatted_wrongAnswer': row['AnswerBText'], 'wrongAnswer': clean_math_text(row['AnswerBText']), 'misconception': misconception_row.MisconceptionName})\n",
    "    if ((row.CorrectAnswer != \"C\") & (math.isnan(row.MisconceptionCId) ==False)):\n",
    "        misconception_text_row = miscon_data[(miscon_data.MisconceptionId==row.MisconceptionCId)]\n",
    "        misconception_id = row.MisconceptionCId\n",
    "        formatted_answer = clean_math_text(row['AnswerCText'])        \n",
    "        # flattened_data.append({'subjectName': row['SubjectName'], 'constructName': row['ConstructName'], 'latex_formatted_question': row['QuestionText'], 'question': clean_math_text(row['QuestionText']), 'latext_formatted_wrongAnswer': row['AnswerCText'], 'wrongAnswer': clean_math_text(row['AnswerCText']), 'misconception': misconception_row.MisconceptionName})\n",
    "    if ((row.CorrectAnswer != \"D\") & (math.isnan(row.MisconceptionDId) == False)):\n",
    "        misconception_text_row = miscon_data[(miscon_data.MisconceptionId==row.MisconceptionDId)]\n",
    "        misconception_id = row.MisconceptionDId\n",
    "        formatted_answer =  clean_math_text(row['AnswerDText'])\n",
    "        # flattened_data.append({'subjectName': row['SubjectName'], 'constructName': row['ConstructName'], 'latex_formatted_question': row['QuestionText'], 'question': clean_math_text(row['QuestionText']), 'latext_formatted_wrongAnswer': row['AnswerDText'], 'wrongAnswer': clean_math_text(row['AnswerDText']), 'misconception': misconception_row.MisconceptionName})\n",
    "    problem_and_wrong_answer = f\"{clean_math_text(row['SubjectName'])} {clean_math_text(row['ConstructName'])} {clean_math_text(row['QuestionText'])} {formatted_answer}\"\n",
    "    misconception_text= misconception_text_row.iloc[0,1]\n",
    "    qa_pairs = []\n",
    "    qa_pairs.append(' '.join(problem_and_wrong_answer.split()[:MAX_LEN]))\n",
    "    qa_pairs.append(' '.join(misconception_text.split()[:MAX_LEN]))\n",
    "    if (index < 10):\n",
    "        print(qa_pairs)\n",
    "    pairs.append(qa_pairs)\n",
    "    misconception_id = misconception_text_row.iloc[0,0]\n",
    "    flattened_data.append({'text': f\"{clean_math_text(row['SubjectName'])} {clean_math_text(row['ConstructName'])} {clean_math_text(row['QuestionText'])} {formatted_answer}\", \"label\": int(misconception_id)})\n",
    "    next_sentence.append({'problem_and_wrong_answer': f\"{clean_math_text(row['SubjectName'])} {clean_math_text(row['ConstructName'])} {clean_math_text(row['QuestionText'])} {formatted_answer}\",\"misconception_text\": misconception_text_row.iloc[0,1], \"misconception_id\": misconception_text_row.iloc[0,0],\"label\":0})\n",
    "    #add_other_misconceptions_as_label_1(problem_and_wrong_answer, misconception_id)\n",
    "flattened_df = pd.DataFrame(flattened_data)\n",
    "# Sort by the 'label' column in ascending order \n",
    "flattened_df = flattened_df.sort_values(by='label')\n",
    "flattened_df = flattened_df.reset_index(drop = True)\n",
    "\n",
    "next_sentence_df = pd.DataFrame(next_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:40:05.440951Z",
     "iopub.status.busy": "2024-11-30T19:40:05.440396Z",
     "iopub.status.idle": "2024-11-30T19:40:05.451690Z",
     "shell.execute_reply": "2024-11-30T19:40:05.450352Z",
     "shell.execute_reply.started": "2024-11-30T19:40:05.440886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "flattened_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T19:41:46.019936Z",
     "iopub.status.busy": "2024-11-30T19:41:46.019467Z",
     "iopub.status.idle": "2024-11-30T19:41:46.033726Z",
     "shell.execute_reply": "2024-11-30T19:41:46.032358Z",
     "shell.execute_reply.started": "2024-11-30T19:41:46.019875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(next_sentence_df.shape)\n",
    "\n",
    "next_sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T20:21:37.152716Z",
     "iopub.status.busy": "2024-11-30T20:21:37.152282Z",
     "iopub.status.idle": "2024-11-30T20:26:04.022094Z",
     "shell.execute_reply": "2024-11-30T20:26:04.020697Z",
     "shell.execute_reply.started": "2024-11-30T20:21:37.152680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentence_a = []\n",
    "sentence_b = []\n",
    "sentence_label = []\n",
    "max_sentence_length = 0\n",
    "for index, row in next_sentence_df.iterrows():\n",
    "    if (len(row['problem_and_wrong_answer']) > max_sentence_length):\n",
    "        max_sentence_length = len(row['problem_and_wrong_answer'])\n",
    "    sentence_a.append(row['problem_and_wrong_answer'])\n",
    "    misconception_id = row['misconception_id']\n",
    "    if (len(row['misconception_text']) > max_sentence_length):\n",
    "        max_sentence_length = len(row['misconception_text'])\n",
    "    sentence_b.append(row['misconception_text'])\n",
    "    sentence_label.append(row['label'])\n",
    "    #print(f\"Index: {index}, problem_and_wrong_answer: {row['problem_and_wrong_answer']}\")\n",
    "\n",
    "print(\"max sentence length\", max_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://towardsdatascience.com/how-to-fine-tune-bert-with-nsp-8b5615468e12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://keras.io/examples/nlp/pretraining_BERT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T20:02:40.111333Z",
     "iopub.status.busy": "2024-11-30T20:02:40.110398Z",
     "iopub.status.idle": "2024-11-30T20:02:56.476391Z",
     "shell.execute_reply": "2024-11-30T20:02:56.475333Z",
     "shell.execute_reply.started": "2024-11-30T20:02:40.111293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import transformers, datasets\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n",
    "TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n",
    "\n",
    "BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n",
    "NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n",
    "SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n",
    "\n",
    "MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-4  # Learning rate for training the model\n",
    "\n",
    "MODEL_CHECKPOINT = \"bert-base-cased\"  # Name of pretrained model from 🤗 Model Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are importing a pre-trained BERT tokenizer and a BERT model with an MLM head from the Hugging Face repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T20:03:55.313165Z",
     "iopub.status.busy": "2024-11-30T20:03:55.312329Z",
     "iopub.status.idle": "2024-11-30T20:03:59.234263Z",
     "shell.execute_reply": "2024-11-30T20:03:59.232821Z",
     "shell.execute_reply.started": "2024-11-30T20:03:55.313121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "#model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Wordpiece tokenizer used for fine-tuning is BertTokenizer. The model used is TFBertForMaskedLM, a BERT model with an MLM head that can accept only Tensorflow tensors. In both of them, the check-point used is bert-base-uncased. Let’s look at the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T20:04:58.252769Z",
     "iopub.status.busy": "2024-11-30T20:04:58.252359Z",
     "iopub.status.idle": "2024-11-30T20:04:58.291153Z",
     "shell.execute_reply": "2024-11-30T20:04:58.289997Z",
     "shell.execute_reply.started": "2024-11-30T20:04:58.252732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece Tokenization\n",
    "\n",
    "The initial stage of creating a fresh BERT model involves training a new tokenizer. Tokenization is the process of breaking down a text into smaller units called “tokens,” which are then converted into a numerical representation. An example of this would be splitting the sentence\n",
    "\n",
    " “I like surfboarding!” → [‘[CLS]’, ‘i’, ‘like’, ‘surf’, ‘##board’, ‘##ing’, ‘!’, ‘[SEP]’] → [1, 48, 250, 4033, 3588, 154, 5, 2]\n",
    "A tokenized BERT input always starts with a special [CLS] token and ends with a special [SEP] token, which are used for specific purposes that will be explained later. BERT employs a WordPiece tokenizer, which can split a single word into multiple tokens. For instance, in the example given earlier, the word “surfboarding” is broken down into ['surf', '##boarding', '##ing']. This technique helps the model to understand that words like surfboardand snowboardhave shared meaning through the common wordpiece ##board. By referring to the explanation from HuggingFace, WordPiece computes a score for each pair, using the following\n",
    "\n",
    "score = (freq_of_pair) / (freq_of_first_element × freq_of_second_element)\n",
    "\n",
    "By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won’t necessarily merge (\"un\", \"##able\") even if that pair occurs very frequently in the vocabulary, because the two pairs \"un\" and \"##able\" will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like (\"hu\", \"##gging\") will probably be merged faster (assuming the word “hugging” appears often in the vocabulary) since \"hu\" and \"##gging\" are likely to be less frequent individually.\n",
    "\n",
    "To train the tokenizer, the BertWordPieceTokenizer from the transformer library was used with the steps below:\n",
    "\n",
    "1. Saving the conversation text into multiple .txt files (with batch of N=10000)\n",
    "2. Define BertWordPieceTokenizer with some parameters likeclean_text to remove control characters, handle_chinese_chars to include spaces around Chinese characters, stripe_accents to remove accents and make é → e, ô → o, andlowercase to view capital and lowercase characters as equal.\n",
    "3. Train the tokenizer based on the file path to .txt files with parameters like vocab_size defines the total number of tokens, min_frequency for minimum frequency for a pair of tokens to be merged, special_tokens defines a list of the special tokens that BERT uses, limit_alphabet for a maximum number of different characters, workpieces_prefix the prefix added to pieces of words (like ##ing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece tokenizer\n",
    "import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "### save data as txt file\n",
    "data_dir = './data'\n",
    "if os.path.isdir(data_dir):\n",
    "    print('directory exists',data_dir)\n",
    "else:\n",
    "    os.mkdir(data_dir)\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm.tqdm([x[0] for x in pairs]):\n",
    "    text_data.append(sample)\n",
    "\n",
    "    # once we hit the 10K mark, save to file\n",
    "    if len(text_data) == 10000:\n",
    "        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "\n",
    "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n",
    "\n",
    "### training own tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer.train( \n",
    "    files=paths,\n",
    "    vocab_size=30_000, \n",
    "    min_frequency=5,\n",
    "    limit_alphabet=1000, \n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
    "    )\n",
    "\n",
    "bert_it_dir = './bert-it-1'\n",
    "if os.path.isdir(bert_it_dir):\n",
    "    print('directory exists',bert_it_dir)\n",
    "else:\n",
    "    os.mkdir(bert_it_dir)\n",
    "\n",
    "tokenizer.save_model(bert_it_dir, 'bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
    "         # Adding PAD token for labels\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input\n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "\n",
    "            # remove cls and sep token\n",
    "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                for i in range(len(token_id)):\n",
    "                    output_label.append(0)\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label\n",
    "\n",
    "    def get_sent(self, index):\n",
    "        '''return random sentence pair'''\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        '''return sentence pair'''\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        '''return random single sentence'''\n",
    "        return self.lines[random.randrange(len(self.lines))][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BERTDataset(\n",
    "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
    "print(len(train_data))\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "sample_data = next(iter(train_loader))\n",
    "print(train_data[random.randrange(len(train_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "The embedding in BERT comprises of three parts, mainly the token embeddings, segment embeddings and position embeddings.\n",
    "\n",
    "\n",
    "In NLP model, the order of the words and their position in a sentence matters and the meaning of the entire sentence can change if the words are re-ordered. As such, transformer model did a position embedding for each token in the input using the formula\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "  - k: Position of an object in input sequence, 0 < k < L/2\n",
    "  - d: Dimension of the output embedding space\n",
    "  - n: User defined scalar. Default by 10,000\n",
    "  - i: Used for mapping to column indices 0 < i < d/2. A single value of i maps to both sine and cosine functions\n",
    "\n",
    "For all three different type of embeddings, they must be in the similar output size (768 in this case), so that all three of them can be summed together to be a single embedded output. You may notice thepadding_idx is specified as 0, this is to make pad token remains as 0 and not being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        for pos in range(max_len):   \n",
    "            # for each dimension of the each position\n",
    "            for i in range(0, d_model, 2):   \n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "        # include the batch size\n",
    "        self.pe = pe.unsqueeze(0)   \n",
    "        # self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe\n",
    "\n",
    "class BERTEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, seq_len=MAX_LEN, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        # (m, seq_len) --> (m, seq_len, embed_size)\n",
    "        # padding_idx is not updated during training, remains as fixed pad (0)\n",
    "        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n",
    "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "       \n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT’s goal is to generate a language representation model, it only needs the encoder part. The code snippet for Multi-head attention might looks complicated at first sight, but it is just a simple translation from the equation below\n",
    "\n",
    "$\n",
    "Z = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$\n",
    "\n",
    "where Q, K, V are identical and linear transformation of input embeddings. The one thing that need more attention is the shape of input tensor, therefore, .permute() function is applied to amend the shape of tensor to fulfil the requirement for dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details of the class MultiHeadedAttention\n",
    "\n",
    "* It’s called multi-head attention because the hidden size: d_model(768) is split by heads(12), this allows the model to jointly attend to information at different positions from different representational spaces.\n",
    "* It takes the query, key, and value as inputs, and the size is permuted from (batch_size, max_len, hidden_size) → (batch_size, num_heads, max_len, hidden_size / num_heads ). This indicates that all the 3 inpurs are linearly projected from the d_model dimensional space to heads sets of d_k dimensional vectors.\n",
    "* Attention score matrix is computed using matrix multiplication between the query(Q) and key(K) tensors, followed by scaling by the square root of the key tensor’s dimension\n",
    "* The mask is applied to the attention matrix and filled with -1e9 (close to negative infinity). This is done because the large negative inputs to softmax are near zero in the output.\n",
    "* The final output is a weighted sum of the value(V) tensors, where the weights are determined by the softmax of the scaled dot-product between the query and key vectors.\n",
    "\n",
    "### The EncoderLayer class contains 2 sublayers:.\n",
    "\n",
    "* MultiHeadedAttention: A multi-headed self-attention module that computes the attention weights between each element in the input sequence\n",
    "* FeedForward: A feedforward network with one hidden layer that applies a non-linear activation function (GELU) to the output of the first linear layer and produces a d_model dimensional output.\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization LayerNorm(x + Sublayer(x)). Residual connections help in avoiding the vanishing gradient problem in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### attention layers\n",
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query = torch.nn.Linear(d_model, d_model)\n",
    "        self.key = torch.nn.Linear(d_model, d_model)\n",
    "        self.value = torch.nn.Linear(d_model, d_model)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, d_model)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    \n",
    "\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        # max_len X max_len matrix of attention\n",
    "        weights = F.softmax(scores, dim=-1)           \n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        # (batch_size, max_len, d_model)\n",
    "        return self.output_linear(context)\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model=768,\n",
    "        heads=12, \n",
    "        feed_forward_hidden=768 * 4, \n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final BERT Model\n",
    "Coming next, we are going to incorporate the encoder layer with attention mechanism into the final BERT’s construction.\n",
    "\n",
    "1. The BERT class initializes the embedding layer for the input sequence, as well as multi layers of EncoderLayer blocks. The forward method of this class takes in the input sequence and a segment info tensor, applies attention masking to the input(for padded token), embeds the input sequence, and then passes it through the encoder blocks to obtain the output.\n",
    "2. The NextSentencePrediction class is a 2-class classification model that takes in the output of the BERT class and predicts whether the input sequence contains two consecutive sentences or not. The forward method applies applies linear transformation and log softmax function to obtain the predicted probabilities of the two classes.\n",
    "3. The MaskedLanguageModel class is a multi-class classification model that takes in the output of the BERT class and predicts the original tokens for the masked input sequence. The forward method applies a linear transformation and log softmax function to obtain the predicted probabilities of each token in the vocabulary.\n",
    "4. The BERTLM class combines the BERT, NextSentencePrediction, and MaskedLanguageModel classes to create a complete BERT language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x\n",
    "\n",
    "class NextSentencePrediction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, 2)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # use only the first token which is the [CLS]\n",
    "        return self.softmax(self.linear(x[:, 0]))\n",
    "\n",
    "class MaskedLanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "class BERTLM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "The original BERT model was trained using Adam optimizer with a custom learning rate scheduler according to the formula in the paper.\n",
    "\n",
    "$\n",
    "lrate = d^{-0.5}_{model} * min(step\\_num^{-0.5},step\\_num * warmup\\_steps^{-1.5})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "We came a long way to finally combine what we have discussed above and start training a new BERT model.\n",
    "\n",
    "The BERTTrainerclass contains train() and test() methods that call the iteration() method to iterate over the given dataloader (train or test) for a specified epoch. The iteration() method calculates the loss and accuracy of the model on the given data and updates the parameters using backpropagation and optimization. It also logs the progress of training with a progress bar and prints the average loss and accuracy at the end of each epoch. Finally, we can do a test run for the BERT model on the processed data with low number of epochs count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        train_dataloader, \n",
    "        test_dataloader=None, \n",
    "        lr= 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "        warmup_steps=10000,\n",
    "        log_freq=10,\n",
    "        device='cuda'\n",
    "        ):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n",
    "            )\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "        \n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        # progress bar\n",
    "        data_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        for i, data in data_iter:\n",
    "\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
    "            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "        print(\n",
    "            f\"EP{epoch}, {mode}: \\\n",
    "            avg_loss={avg_loss / len(data_iter)}, \\\n",
    "            total_acc={total_correct * 100.0 / total_element}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''test run'''\n",
    "\n",
    "train_data = BERTDataset(\n",
    "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "bert_model = BERT(\n",
    "  vocab_size=len(tokenizer.vocab),\n",
    "  d_model=768,\n",
    "  n_layers=2,\n",
    "  heads=12,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
    "bert_trainer = BERTTrainer(bert_lm, train_loader, device='cpu')\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  bert_trainer.train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build MLM inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=max_sentence_length, truncation=True, padding='max_length')\n",
    "inputs.token_type_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create class label — The next step is easy, all we need to do here is create a new labels tensor that identifies whether sentence B follows sentence A.\n",
    "\n",
    "We use a value of 0 to represent IsNextSentence and 1 for NotNextSentence. Additionally, we must use the torch.LongTensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#inputs['label'] = torch.LongTensor([sentence_label]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate loss — Finally, we get around to calculating our loss. We start by processing our inputs and labels through our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-30T19:38:05.093075Z",
     "iopub.status.idle": "2024-11-30T19:38:05.093614Z",
     "shell.execute_reply": "2024-11-30T19:38:05.093356Z",
     "shell.execute_reply.started": "2024-11-30T19:38:05.093329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-30T19:38:05.095489Z",
     "iopub.status.idle": "2024-11-30T19:38:05.095996Z",
     "shell.execute_reply": "2024-11-30T19:38:05.095792Z",
     "shell.execute_reply.started": "2024-11-30T19:38:05.095765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(outputs.loss)\n",
    "#print(outputs.loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will return the loss tensor, which is what we would optimize on during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction\n",
    "We may also not need to train our model, and would just like to use the model for inference. In this case, we would have no labels tensor, and we would modify the last part of our code to extract the logits tensor like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-30T19:38:05.097526Z",
     "iopub.status.idle": "2024-11-30T19:38:05.097869Z",
     "shell.execute_reply": "2024-11-30T19:38:05.097719Z",
     "shell.execute_reply.started": "2024-11-30T19:38:05.097703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#outputs = model(**inputs)\n",
    "#outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And take the argmax to get our prediction:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-30T19:38:05.099495Z",
     "iopub.status.idle": "2024-11-30T19:38:05.100112Z",
     "shell.execute_reply": "2024-11-30T19:38:05.099741Z",
     "shell.execute_reply.started": "2024-11-30T19:38:05.099716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#torch.argmax(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will return a logits tensor, which contains two values — the activation for the IsNextSentence class in index 0, and the activation for the NotNextSentence class in index 1.\n",
    "\n",
    "From here, all we do is take the argmax of the output logits to return our model’s prediction. In this case, it returns 0 — meaning BERT believes sentence B does follow sentence A (correct)."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
